{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subtle-input",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=hQ1oK0kXaV5p\n",
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"antoiloui/belgpt2\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"antoiloui/belgpt2\", model_max_length=768, pad_token='<|pad|>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noted-peninsula",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\t', ' '), ('A : ', 'A: '), (' A: ', '\\nA: '), ('B : ', 'B: '), (' B: ', '\\nB: '), ('C : ', 'C: '), (' C: ', '\\nC: ')]\n",
      "B: Dieu? Ã‰coutez, je ne veux pas vous vexer, mais je ne crois pas que Dieu existe.\n",
      "A: Vous ne croyez pas que Dieu existe? Comment cela?\n",
      "5090210\n",
      "1678\n"
     ]
    }
   ],
   "source": [
    "replace = [ \n",
    "    ( '\\t', ' ')\n",
    "]\n",
    "\n",
    "# until line 1645 there are dialogues\n",
    "# lets split it there\n",
    "# then creating pairs or triplets or 4lets or whatever -> check length\n",
    "# for the model\n",
    "# using the rest of the text just as a normal finetune training set\n",
    "\n",
    "for c in ['A', 'B', 'C']:\n",
    "    # get rid of weird whitespace\n",
    "    replace.append(('{} : '.format(c), '{}: '.format(c)))\n",
    "    # adding missing newlines\n",
    "    replace.append((' {}: '.format(c), '\\n{}: '.format(c)))\n",
    "    \n",
    "print(replace)\n",
    "\n",
    "# load dataset and split into smaller texts\n",
    "with open('datasets/training-raw.txt', 'r') as raw_file:\n",
    "    text = raw_file.read()\n",
    "    for t,r in replace:\n",
    "        text = text.replace(t, r)\n",
    "split_text = text.split('\\n\\n')\n",
    "# dialogs end after 1678\n",
    "split_after_dialog = text.split('\\n')[:1678]\n",
    "just_text = list(filter(None, text.split('\\n')[1678:]))\n",
    "\n",
    "# [::2] samples every second item, [1::2] starts at index one\n",
    "\n",
    "pairs = map( lambda x: '\\n'.join(x), zip(split_after_dialog[::2], split_after_dialog[1::2]))\n",
    "pairs = list(pairs)\n",
    "\n",
    "print(pairs[1])\n",
    "print(len(text))\n",
    "print(len(split_after_dialog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "overall-peter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50258\n",
      "50257\n",
      "{'input_ids': [174, 122, 126, 8216, 1702, 559, 1184, 295, 30959, 337, 338, 1603, 303, 24654, 16, 5079, 15, 352, 267, 161, 225, 250, 4029, 2622, 297, 37017, 1357, 354, 287, 31215, 1710, 338, 6392, 16, 980, 866, 287, 1603, 303, 24654, 11805, 14, 287, 1702, 385, 6177, 364, 287, 7281, 16, 41634, 39947, 14, 581, 2884, 451, 1589, 15, 36957, 293, 9499, 295, 30959, 16, 201, 35, 28, 4516, 1159, 14, 585, 1346, 16, 859, 354, 2394, 472, 13941, 4583, 3, 201, 36, 28, 2394, 33, 41276, 14, 475, 423, 3496, 380, 472, 34217, 274, 14, 489, 475, 423, 2762, 380, 354, 2394, 2723, 16, 201, 35, 28, 1146, 423, 15113, 380, 354, 2394, 2723, 33, 2918, 1061, 33, 201, 36, 28, 13557, 14, 268, 161, 225, 250, 319, 2065, 16, 897, 299, 161, 225, 250, 67, 289, 161, 225, 250, 616, 3429, 364, 287, 2386, 337, 1205, 354, 2394, 299, 161, 225, 250, 4029, 828, 380, 16, 26075, 15, 6125, 14, 506, 2394, 2723, 14, 2337, 524, 266, 15, 86, 15, 296, 1499, 311, 1773, 9313, 33, 5301, 743, 622, 1313, 23261, 33, 5301, 307, 10011, 364, 295, 941, 33, 5301, 743, 622, 23463, 14, 1049, 622, 37007, 33, 437, 161, 225, 250, 1476, 333, 863, 301, 7660, 292, 2394, 369, 842, 1049, 622, 2064, 16, 3773, 14, 601, 10148, 1719, 9320, 14, 15113, 15, 6125, 14, 2394, 299, 161, 225, 250, 4029, 828, 380, 3, 201, 6961, 481, 8502, 14, 287, 31215, 267, 161, 225, 250, 26818, 938, 687, 765, 292, 2275, 16, 942, 10538, 292, 1262, 497, 4653, 303, 5169, 293, 301, 287, 16276, 10755, 388, 16, 488, 31215, 11239, 1205, 295, 30959, 16, 201, 35, 28, 1146, 7292, 2160, 33, 520, 9605, 1376, 299, 161, 225, 250, 4029, 12822, 380, 3, 201, 36, 28, 10736, 3, 13923, 3, 2918, 1029, 14, 307, 9605, 1376, 299, 161, 225, 250, 4029, 12822, 380, 33, 851, 161, 225, 250, 319, 15, 325, 354, 472, 483, 2297, 2512, 33, 1146, 303, 1578, 292, 848, 307, 2362, 3, 40295, 3, 201, 35, 28, 3773, 3, 1347, 299, 9, 4029, 12822, 380, 16, 367, 161, 225, 250, 1174, 29592, 14, 501, 423, 28849, 380, 16245, 364, 307, 5893, 311, 1647, 497, 4653, 303, 5169, 293, 301, 287, 16276, 10755, 388, 270, 540, 1182, 1262, 15, 2770, 3, 398, 15911, 2114, 295, 4730, 11, 19665, 3, 201, 2532, 30959, 6177, 329, 287, 6962, 16, 201, 36, 28, 6805, 3, 794, 374, 1065, 2933, 409, 622, 1773, 1252, 423, 4434, 380, 1071, 1167, 3, 201, 35, 28, 36847, 3, 476, 1065, 2933, 409, 2394, 2723, 3, 794, 307, 1773, 423, 2342, 380, 1071, 648, 293, 423, 295, 10523, 380, 16, 6051, 2337, 374, 524, 266, 1499, 269, 10223, 364, 295, 941, 3, 201, 2532, 6025, 559, 303, 2482, 16, 201, 37, 28, 3017, 107, 40159, 6628, 15, 1520, 269, 2394, 14, 293, 374, 267, 9, 2143, 316, 715, 67, 269, 472, 16, 3017, 122, 201, 35, 28, 332, 161, 225, 250, 319, 1243, 354, 295, 1864, 266, 7715, 33, 201, 35, 28, 24896, 161, 225, 250, 286, 266, 15, 86, 15, 296, 2633, 295, 1864, 33, 201, 35, 28, 3099, 15, 325, 354, 268, 161, 225, 250, 319, 1243, 354, 295, 1864, 266, 551, 17437, 33, 201, 35, 28, 31816, 15, 1520, 9480, 289, 161, 225, 250, 792, 24215, 266, 3558, 293, 578, 1402, 1330, 33, 201, 35, 28, 15607, 524, 716, 15, 296, 269, 9123, 33, 201, 2532, 1017, 288, 546, 14379, 287, 963, 337, 627, 17425, 307, 5692, 16, 201, 35, 28, 3099, 15, 325, 1243, 354, 287, 1702, 333, 10058, 266, 4994, 292, 2177, 364, 262, 161, 225, 250, 8992, 422, 33, 201, 35, 28, 3099, 15, 325, 354, 268, 161, 225, 250, 319, 1243, 354, 287, 1702, 333, 10058, 266, 23748, 292, 12142, 33, 201, 35, 28, 5301, 295, 10058, 299, 161, 225, 250, 319, 15, 296, 380, 1375, 337, 4944, 415, 369, 327, 5524, 33, 201, 35, 28, 31816, 15, 1520, 522, 765, 301, 684, 3502, 295, 1864, 266, 551, 29075, 75, 33, 201, 35, 28, 3099, 15, 325, 1243, 354, 295, 9315, 6773, 716, 23748, 463, 1999, 24215, 33, 201, 36, 28, 42341, 3, 398, 5335, 2188, 354, 295, 3424, 267, 161, 225, 250, 514, 1439, 284, 16, 11, 642, 282, 161, 225, 250, 2143, 449, 2127, 933, 310, 16, 642, 1057, 292, 1017, 288, 546, 269, 287, 3246, 333, 7045, 269, 1897, 31787, 295, 10058, 269, 287, 4809, 269, 16069, 16, 450, 10058, 17352, 1044, 282, 161, 225, 250, 67, 14905, 269, 9649, 292, 7693, 4668, 301, 287, 2852, 16, 4940, 864, 161, 225, 102, 398], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "train_encodings = tokenizer(text, truncation=True)\n",
    "print(train_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "right-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (877 > 768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.94874851013111\n",
      "613\n",
      "1\n",
      "----\n",
      "1114.2517605633802\n",
      "941333\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "doc_lengths = []\n",
    "\n",
    "for pair in pairs:\n",
    "    # get rough token count distribution\n",
    "    tokens = tokenizer(pair)\n",
    "\n",
    "    doc_lengths.append(len(tokens['input_ids']))\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "print(np.average(doc_lengths))\n",
    "print(np.max(doc_lengths))\n",
    "print(np.min(doc_lengths))\n",
    "\n",
    "# the dialogs are okay because they are shorter\n",
    "# then the max length\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "text_lengths = []\n",
    "\n",
    "for t in just_text:\n",
    "    # get rough token count distribution\n",
    "    tokens = tokenizer(t)\n",
    "    if len(tokens['input_ids']) == 0:\n",
    "        print(t)\n",
    "\n",
    "    text_lengths.append(len(tokens['input_ids']))\n",
    "\n",
    "doc_lengths = np.array(text_lengths)\n",
    "print(np.average(text_lengths))\n",
    "print(np.max(text_lengths))\n",
    "print(np.min(text_lengths))\n",
    "\n",
    "# the normal texts are too long\n",
    "# we need to split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "corporate-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251\n",
      "True\n",
      "69.98880895283773\n",
      "252\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "max_length = 768\n",
    "split_french = []\n",
    "\n",
    "for f in just_text:\n",
    "    split_french.append( [j for j in (f[i:i + max_length] for i in  range(0, len(just_text), max_length))])\n",
    "\n",
    "# split_french = filter(None, split_french)\n",
    "split_french = np.array(split_french).flatten()\n",
    "split_french = [ x for x in split_french if x != '']\n",
    "\n",
    "text_lengths = []\n",
    "\n",
    "for t in split_french:\n",
    "    # get rough token count distribution\n",
    "    tokens = tokenizer(t)\n",
    "    if len(tokens['input_ids']) == 0:\n",
    "        print(\"empty {}\".format(t))\n",
    "    if t == '':\n",
    "        print(\"empty string {}\".format(t))\n",
    "\n",
    "    text_lengths.append(len(tokens['input_ids']))\n",
    "\n",
    "print(len(split_french))\n",
    "print(len(split_french) > len(just_text))\n",
    "doc_lengths = np.array(text_lengths)\n",
    "print(np.average(text_lengths))\n",
    "print(np.max(text_lengths))\n",
    "print(np.min(text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "assigned-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "class FrenchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        \n",
    "        for text in texts:\n",
    "            encodings_dict = tokenizer(tokenizer.bos_token + text + tokenizer.eos_token, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #  {\"input_ids\": [...], \"labels\": [...], \"attention_mask\": [..]}\n",
    "        return { \"input_ids\": self.input_ids[idx], \"labels\": self.input_ids[idx], \"attention_mask\": self.attn_masks[idx] }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "middle-freight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_dataset = FrenchDataset(split_french, tokenizer)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(french_dataset))\n",
    "val_size = len(french_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(french_dataset, [train_size, val_size])\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accomplished-movie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='563' max='563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [563/563 10:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.340300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.338500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.389500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.302200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.371600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.254600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.326800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.263900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=563, training_loss=0.9867052759306892, metrics={'train_runtime': 628.6061, 'train_samples_per_second': 0.896, 'total_flos': 645103927296000.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': -154378240, 'init_mem_gpu_alloc_delta': 510354432, 'init_mem_cpu_peaked_delta': 154378240, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 11145216, 'train_mem_gpu_alloc_delta': 1493839360, 'train_mem_cpu_peaked_delta': 462319616, 'train_mem_gpu_peaked_delta': 5273303040})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=2,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "growing-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del trainer\n",
    "del training_args\n",
    "del model\n",
    "del tokenizer\n",
    "del val_dataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-brazil",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

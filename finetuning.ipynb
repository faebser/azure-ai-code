{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "desperate-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=hQ1oK0kXaV5p\n",
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"antoiloui/belgpt2\")\n",
    "#GPT2LMHeadModel.from_pretrained(\"results/checkpoint-3500/\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"antoiloui/belgpt2\", model_max_length=768, pad_token='<|pad|>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sacred-syndication",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\t', ' '), ('A : ', 'A: '), (' A: ', '\\nA: '), ('B : ', 'B: '), (' B: ', '\\nB: '), ('C : ', 'C: '), (' C: ', '\\nC: ')]\n",
      "A: Vous ne croyez pas que Dieu existe ?  Comment cela ? \n",
      "B: C’est simple. On n’a qu’à sortir dans la rue pour voir que Dieu n’existe pas. Dites-moi, si Dieu existe, pourquoi y A :t-il tant des gens malades ?  Pourquoi tous ces enfants abandonnés ?  Pourquoi les guerres dans le monde ?  Pourquoi tous ces malheurs, toutes ces misères ?  J’ai du mal à imaginer un Dieu qui permet toutes ces choses. Non, ma chère petite madame. Un peu triste, la cliente s’éloigne sans dire un mot. Elle croise un homme aux cheveux en bataille et à la barbe négligée. La cliente retourne voir le coiffeur.\n",
      "B:  Mais oui ! La vie éternelle ! \n"
     ]
    }
   ],
   "source": [
    "replace = [ \n",
    "    ( '\\t', ' ')\n",
    "]\n",
    "\n",
    "# until line 1645 there are dialogues\n",
    "# lets split it there\n",
    "# then creating pairs or triplets or 4lets or whatever -> check length\n",
    "# for the model\n",
    "# using the rest of the text just as a normal finetune training set\n",
    "\n",
    "for c in ['A', 'B', 'C']:\n",
    "    # get rid of weird whitespace\n",
    "    replace.append(('{} : '.format(c), '{}: '.format(c)))\n",
    "    # adding missing newlines\n",
    "    replace.append((' {}: '.format(c), '\\n{}: '.format(c)))\n",
    "    \n",
    "print(replace)\n",
    "\n",
    "# load dataset and split into smaller texts\n",
    "with open('datasets/data-raw-2.txt', 'r') as raw_file:\n",
    "    text = raw_file.read()\n",
    "    for t,r in replace:\n",
    "        text = text.replace(t, r)\n",
    "split_text = text.split('\\n\\n')\n",
    "# dialogs end after 1678\n",
    "# dialogs end after 1563 in raw2\n",
    "split_after_dialog = text.split('\\n')[:1563]\n",
    "just_text = list(filter(None, text.split('\\n')[1678:]))\n",
    "\n",
    "# [::2] samples every second item, [1::2] starts at index one\n",
    "\n",
    "pairs = map( lambda x: '\\n'.join(x), zip(split_after_dialog[::2], split_after_dialog[1::2]))\n",
    "pairs = list(pairs)\n",
    "\n",
    "print(pairs[1])\n",
    "print(split_after_dialog[-1])\n",
    "#print(len(split_after_dialog))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italic-shoot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50258\n",
      "50257\n",
      "{'input_ids': [174, 122, 126, 8216, 1702, 559, 1184, 295, 30959, 337, 338, 1603, 303, 24654, 16, 5079, 15, 352, 267, 161, 225, 250, 4029, 2622, 297, 37017, 1357, 354, 287, 31215, 1710, 338, 6392, 16, 980, 866, 287, 1603, 303, 24654, 11805, 14, 287, 1702, 385, 6177, 364, 287, 7281, 16, 41634, 39947, 14, 581, 2884, 451, 1589, 15, 36957, 293, 9499, 295, 30959, 16, 223, 201, 35, 28, 4516, 1159, 14, 585, 1346, 16, 859, 354, 2394, 472, 13941, 4583, 576, 223, 201, 35, 28, 1146, 423, 15113, 380, 354, 2394, 2723, 632, 223, 2918, 1061, 632, 223, 201, 36, 28, 332, 161, 225, 250, 319, 2065, 16, 897, 299, 161, 225, 250, 67, 289, 161, 225, 250, 616, 3429, 364, 287, 2386, 337, 1205, 354, 2394, 299, 161, 225, 250, 4029, 828, 380, 16, 26075, 15, 6125, 14, 506, 2394, 2723, 14, 2337, 524, 358, 409, 86, 15, 296, 1499, 311, 1773, 9313, 632, 223, 5301, 743, 622, 1313, 23261, 632, 223, 5301, 307, 10011, 364, 295, 941, 632, 223, 5301, 743, 622, 23463, 14, 1049, 622, 37007, 632, 223, 437, 161, 225, 250, 1476, 333, 863, 301, 7660, 292, 2394, 369, 842, 1049, 622, 2064, 16, 3773, 14, 601, 10148, 1719, 9320, 16, 777, 481, 8502, 14, 287, 31215, 267, 161, 225, 250, 26818, 938, 687, 765, 292, 2275, 16, 942, 10538, 292, 1262, 497, 4653, 303, 5169, 293, 301, 287, 16276, 10755, 388, 16, 488, 31215, 11239, 1205, 295, 30959, 16, 201, 35, 28, 2918, 1143, 290, 161, 225, 250, 2143, 754, 632, 223, 223, 201, 36, 28, 475, 282, 161, 225, 250, 2143, 449, 320, 19621, 16, 642, 1057, 2394, 16, 642, 1057, 292, 7524, 576, 223, 201, 35, 28, 4177, 575, 15, 419, 632, 223, 201, 36, 28, 642, 1057, 320, 19621, 16, 642, 1057, 2394, 16, 642, 1057, 3830, 16, 201, 35, 28, 36847, 576, 476, 1065, 2933, 409, 2394, 2723, 576, 794, 307, 1773, 423, 2342, 380, 1071, 648, 293, 423, 295, 10523, 380, 16, 6051, 2337, 374, 524, 266, 1499, 269, 10223, 364, 295, 941, 576, 223, 201, 37, 28, 40159, 6628, 15, 1520, 269, 2394, 14, 293, 374, 267, 9, 2143, 316, 715, 67, 269, 472, 16, 223, 201, 35, 28, 2321, 575, 15, 419, 632, 223, 201, 36, 28, 223, 642, 1057, 3830, 16, 642, 1057, 364, 4341, 2362, 16, 16745, 16, 201, 35, 28, 1445, 1143, 290, 161, 225, 250, 2143, 754, 632, 223, 223, 201, 35, 28, 642, 282, 161, 225, 250, 2143, 449, 320, 19621, 632, 223, 201, 35, 28, 15607, 524, 716, 15, 296, 269, 9123, 632, 223, 201, 2532, 1017, 288, 546, 14379, 287, 963, 337, 627, 17425, 307, 5692, 16, 201, 35, 28, 3099, 15, 325, 1243, 354, 287, 1702, 333, 10058, 266, 4994, 292, 2177, 364, 262, 161, 225, 250, 8992, 422, 632, 223, 201, 35, 28, 3099, 15, 325, 354, 268, 161, 225, 250, 319, 1243, 354, 287, 1702, 333, 10058, 266, 23748, 292, 12142, 632, 223, 201, 35, 28, 5301, 295, 10058, 299, 161, 225, 250, 319, 15, 296, 380, 1375, 337, 4944, 415, 369, 327, 5524, 632, 223, 201, 35, 28, 31816, 15, 1520, 522, 765, 301, 684, 3502, 295, 1864, 266, 551, 29075, 75, 632, 223, 201, 35, 28, 3099, 15, 325, 1243, 354, 295, 9315, 6773, 716, 23748, 463, 1999, 24215, 632, 223, 201, 36, 28, 42341, 576, 642, 282, 161, 225, 250, 2143, 449, 2127, 933, 310, 16, 642, 1057, 292, 1017, 288, 546, 269, 287, 3246, 333, 7045, 269, 1897, 31787, 295, 10058, 269, 287, 4809, 269, 16069, 16, 450, 10058, 17352, 1044, 282, 161, 225, 250, 67, 14905, 269, 9649, 292, 7693, 4668, 301, 287, 2852, 16, 4940, 864, 16, 223, 777, 34510, 4869, 5388, 269, 49185, 266, 551, 19493, 329, 40213, 546, 374, 524, 266, 939, 1226, 16, 1897, 1864, 266, 551, 29075, 75, 364, 295, 20666, 259, 161, 225, 250, 286, 15747, 333, 10852, 13100, 5222, 767, 463, 1717, 16, 770, 2221, 14, 1913, 821, 295, 8075, 333, 3522, 14, 295, 1864, 266, 551, 11164, 17437, 329, 307, 17594, 37941, 333, 34510, 14, 337, 5257, 269, 627, 4017, 301, 287, 24215, 354, 295, 34510, 716, 648, 15, 1839, 22971, 31692, 16, 1434, 262, 161, 225, 250, 3396, 4787, 14, 522, 299, 161, 225, 250, 923, 407, 380, 799, 2633, 295, 1864, 16, 3348, 14, 1735, 354, 522, 262, 161, 225, 250, 2308, 407, 2633, 14, 522, 295, 50128, 301, 287, 3598, 269, 287, 2852, 1377, 354, 415, 1192, 27233, 979, 13978, 16, 223, 332, 161, 225, 250, 319, 560, 415, 354, 335, 161, 225, 250, 1476, 301, 765, 337, 295, 1324], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "train_encodings = tokenizer(text, truncation=True)\n",
    "print(train_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mighty-undergraduate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.302176696542894\n",
      "1180\n",
      "6\n",
      "----\n",
      "2817.012578616352\n",
      "383808\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "doc_lengths = []\n",
    "\n",
    "for pair in pairs:\n",
    "    # get rough token count distribution\n",
    "    tokens = tokenizer(pair)\n",
    "\n",
    "    doc_lengths.append(len(tokens['input_ids']))\n",
    "\n",
    "doc_lengths = np.array(doc_lengths)\n",
    "print(np.average(doc_lengths))\n",
    "print(np.max(doc_lengths))\n",
    "print(np.min(doc_lengths))\n",
    "\n",
    "# the dialogs are okay because they are shorter\n",
    "# then the max length\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "text_lengths = []\n",
    "\n",
    "for t in just_text:\n",
    "    # get rough token count distribution\n",
    "    tokens = tokenizer(t)\n",
    "    if len(tokens['input_ids']) == 0:\n",
    "        print(t)\n",
    "\n",
    "    text_lengths.append(len(tokens['input_ids']))\n",
    "\n",
    "doc_lengths = np.array(text_lengths)\n",
    "print(np.average(text_lengths))\n",
    "print(np.max(text_lengths))\n",
    "print(np.min(text_lengths))\n",
    "\n",
    "# the normal texts are too long\n",
    "# we need to split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "devoted-rating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je désire Te voir couronné, portant la massue et le disque dans Ta main. Par conséquent, O Seigneur aux milliers de bras et la forme universelle, apparais je t’en supplie avec Ta forme à quatre bras. Le Suprême Seigneur dit : , étant satisfait de toi, Je t’ai montré par Mes propres forces yoguiques, cette forme suprême, lumineuse, universelle, infinie, et primordiale de Moi, et qui avant n’a jamais été vue par un autre que toi. , ni l’étude des Védas, ni les sacrifices, ni la charité, ni les rituels, ni les austérités sévères permettent à quiconque de Me voir dans cette forme cosmique, sauf nul autre que toi dans ce monde humain. \n",
      "False\n",
      "50.138364779874216\n",
      "247\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "max_length = 768\n",
    "split_french = []\n",
    "\n",
    "for f in just_text:\n",
    "    split_french.append( [j for j in (f[i:i + max_length] for i in  range(0, len(just_text), max_length))])\n",
    "\n",
    "# split_french = filter(None, split_french)\n",
    "split_french = np.array(split_french).flatten()\n",
    "split_french = [ x for x in split_french if x != '']\n",
    "\n",
    "text_lengths = []\n",
    "\n",
    "for t in split_french:\n",
    "    # get rough token count distribution\n",
    "    tokens = tokenizer(t)\n",
    "    if len(tokens['input_ids']) == 0:\n",
    "        print(\"empty {}\".format(t))\n",
    "    if t == '':\n",
    "        print(\"empty string {}\".format(t))\n",
    "\n",
    "    text_lengths.append(len(tokens['input_ids']))\n",
    "\n",
    "print(split_french[10])\n",
    "print(len(split_french) > len(just_text))\n",
    "doc_lengths = np.array(text_lengths)\n",
    "print(np.average(text_lengths))\n",
    "print(np.max(text_lengths))\n",
    "print(np.min(text_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "searching-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "class FrenchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        \n",
    "        for text in texts:\n",
    "            encodings_dict = tokenizer(tokenizer.bos_token + text + tokenizer.eos_token, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #  {\"input_ids\": [...], \"labels\": [...], \"attention_mask\": [..]}\n",
    "        return { \"input_ids\": self.input_ids[idx], \"labels\": self.input_ids[idx], \"attention_mask\": self.attn_masks[idx] }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "linear-anger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_dataset = FrenchDataset(split_french, tokenizer)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(french_dataset))\n",
    "val_size = len(french_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(french_dataset, [train_size, val_size])\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "synthetic-arizona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'compute_37']\n",
      "_CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.cuda.get_arch_list())\n",
    "import torch\n",
    "print(torch.cuda.get_device_properties(torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cardiac-louisville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x7f60029e0050>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c19947dab65b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    919\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    758\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m                 )\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results-2',          # output directory\n",
    "    num_train_epochs=11,              # total number of training epochs\n",
    "    per_device_train_batch_size=3,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    #no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ready-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del trainer\n",
    "del training_args\n",
    "del model\n",
    "del tokenizer\n",
    "del val_dataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "different-easter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"La fondation de la cité  s' accomplit par la consécration d' une âme à Dieu, en coïncidant avec la création. Toute protestation devient ainsi un déni de réalité. Les dieux védiques font aussi remarquer l‟appréciable\", 'La fondation de la cité  transcende les différences individuelles et sociales. L‟ensemble des constructions matérielles appartient à l‟ensemble des modes de développement de l‟individu, aussi Plus en détail L. A.']\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "model_test = GPT2LMHeadModel.from_pretrained(\"results/checkpoint-3500/\")\n",
    "tokenizer_test = GPT2TokenizerFast.from_pretrained(\"antoiloui/belgpt2\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "prompt = tokenizer_test.bos_token + \"La fondation de la cité \"\n",
    "inputs = tokenizer_test.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "model_test.cuda()\n",
    "\n",
    "# Generate a sample of text\n",
    "model_test.eval()\n",
    "output = model_test.generate(\n",
    "            inputs,\n",
    "            do_sample=True,   \n",
    "            top_k=50, \n",
    "            max_length=50,\n",
    "            top_p=0.95, \n",
    "            num_return_sequences=2\n",
    ")\n",
    "\n",
    "# Decode it\n",
    "decoded_output = []\n",
    "for sample in output:\n",
    "    decoded_output.append(tokenizer_test.decode(sample, skip_special_tokens=True))\n",
    "print(decoded_output)\n",
    "\n",
    "del model_test\n",
    "del tokenizer_test\n",
    "del device\n",
    "del inputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "clean-blackberry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50259, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 768\n",
    "pairs_dataset = FrenchDataset(pairs, tokenizer)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(pairs_dataset))\n",
    "val_size = len(pairs_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(pairs_dataset, [train_size, val_size])\n",
    "# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "independent-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2520' max='2520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2520/2520 1:02:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.348600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.332100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.272400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.185100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.182100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.165300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.114800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.122100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.147400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.158700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.121000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.099900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.104800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.124700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.088500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.094400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.077300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.105300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.055800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.060900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.059700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.056900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.063400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.049700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2520, training_loss=0.12986654905336245, metrics={'train_runtime': 3759.9879, 'train_samples_per_second': 0.67, 'total_flos': 4329364134297600.0, 'epoch': 10.0, 'init_mem_cpu_alloc_delta': 164855808, 'init_mem_gpu_alloc_delta': 511148032, 'init_mem_cpu_peaked_delta': 497131520, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 158498816, 'train_mem_gpu_alloc_delta': 1494403584, 'train_mem_cpu_peaked_delta': 463024128, 'train_mem_gpu_peaked_delta': 7968615424})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results-finetune',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=3,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "recovered-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del trainer\n",
    "del training_args\n",
    "del model\n",
    "del tokenizer\n",
    "del val_dataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "capital-miracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A: Le fondement de la ville, ce sont ses habitants.\\n B:  C’est son histoire, ou plutôt… ses habitants…', 'A: Le fondement de la ville, ce sont ses habitants.\\n B:  C’est vrai qu’il est bâti à même le mur de Jérusalem! Si Jérusalem est minuscule, c’est vrai!']\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "model_test = GPT2LMHeadModel.from_pretrained(\"results-finetune/checkpoint-2500/\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"antoiloui/belgpt2\", model_max_length=768, pad_token='<|pad|>')\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "prompt = tokenizer.bos_token + \"A: Le fondement de la ville, ce sont ses habitants.\\n B: \"\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "model_test.cuda()\n",
    "\n",
    "# Generate a sample of text\n",
    "model_test.eval()\n",
    "output = model_test.generate(\n",
    "            inputs,\n",
    "            do_sample=True,   \n",
    "            top_k=50, \n",
    "            max_length=50,\n",
    "            top_p=0.95, \n",
    "            num_return_sequences=2\n",
    ")\n",
    "\n",
    "# Decode it\n",
    "decoded_output = []\n",
    "for sample in output:\n",
    "    decoded_output.append(tokenizer.decode(sample, skip_special_tokens=True))\n",
    "print(decoded_output)\n",
    "\n",
    "del model_test\n",
    "del device\n",
    "del inputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "quiet-venezuela",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe-t-il un seul Dieu?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = []\n",
    "\n",
    "with open('datasets/questions.txt', 'r') as raw_file:\n",
    "    for l in raw_file:\n",
    "        questions.append(l)\n",
    "\n",
    "print(questions[5])\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-trademark",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C’est lui, mon frère, qui m’a transmis l’eau que Dieu m’a donnée. Pourquoi le vide me donne la vue…', 'Il n’y a pas eu d’étre fixe pendant ces guerres. Ce qui m’étonne le plus, c’est plutôt l’éternel bonté qu’il a mise sur moi, parce que je suis habituée de voir les choses telles qu’elles sont…']\n",
      "['Ça fait longtemps?', 'Où suis-tu?']\n",
      "['Je viens de l’inventer. Je sais ce que tu veux dire. Je ne te l’ai jamais… mais j’ai l’aimerais bien savoir ce que tu penses de moi. Tout le monde est en place, chacun s’adresse à moi, et moi je vais répondre à leurs questions.', 'Qu’est-ce que tu veux dire?']\n",
      "['Je sais. Mais je ne t’écoute pas. Qu’est-ce que je vais lui dire?', 'Ça semble avoir marché pendant un certain temps. Je me souviens d’avoir poussé un cri, d’avoir poussé un cri, d’avoir arraché un soupir. Mais dès que j’ai poussé un cri, je suis rentrée chez moi et j’ai consulté mes souvenirs et mes prières. Je me suis lavé, je me suis relevée, j’ai']\n",
      "['En fait, on ne sait pas ce qui se passeestoù il veut s’endormir.', 'Un enfer à eux tout seuls?']\n",
      "['Mon peuple ne m’a jamais parlé. Je ne vois pas d’homme qui pourrait m’aider.', 'Oui, Dieu existe. Je n’ai jamais vu un Dieu vivant.']\n",
      "['Tu es tannée, hein?', 'Pourquoi suis-tu si méchant?']\n",
      "['Oui, tout le monde a ce qu’ils cherchent. Mais ce qu’ils cherchent, ce sont les réponses à leurs questions.', 'Non, ce n’existe pas. Moi, je n’existe pas.']\n",
      "['Ce n’est pas moi, l’aveugle…', 'Ouais, pas mal, mais quand même!']\n",
      "['S’il n’y a pas de variation, il y a seulement deux choses qui se sont passées : d’abor ses habitudes de vie et son état d’anesthésie, ou bien il essaie d’en changer et, se trompant de chose, il choisit de garder l’air simplement là où il veut, en sachant qu', '(Lentement.) À Dieu? C’est Dieu qui ouvre les yeux du pauvre D.']\n",
      "['(Acclamations) Je suis ton propre père.', 'Mon mari est chef d’Israël. (Silence.) Ça y est, Jean, des femmes de la ville accourent. Dieu les prend en photo et elles sont vues de loin.']\n",
      "['(Levant les mains en signe d’incompréhension) Eh bien, à ce point-ci, je ne peux que vous poser une question : « Pouvez-vous nous dire avec certitude qu’il s’est vraiment présenté à l’Kent? »?', '(Pause) Le soleil est haut dans le ciel! Pourquoi le soleil est haut dans le ciel?']\n",
      "['Légère déception?', '(Avec fierté) À la demande, papa va à la porte, ouvre et voit Jésus. Il voit Jésus.']\n",
      "['Je veux vivre, je t’aime, j’en suis sûr.', 'Je pense que c’est par là que le livreur est parti. Ce n’est pas là son premier miracle. Il a été guéri deux fois : d’abor ses mains et son cœur.']\n",
      "['Je ne l’ai pas. Je l’ai donnée à ma petite A, qui est très jolie. Je la donne à ma grande B qui est très malheureuse.', 'Je ne l’ai pas. Je n’ai pas l’impression. Je fais confiance au travail de ma mère.']\n",
      "['Je vous l’ai expliqué tout à l’heure, moi, parce que j’étais convaincue que Dieu existe!', 'Oui, je fais partie de l’Association des sceptiques… Ce n’est pas nous qui avons créé le Drakenskāra, c’est vous qui nous l’envoilà?']\n",
      "['Oui. C’est pourquoi je pense que tu dois être certaine de ne pas me faire de faux positifs.', '(Apercevant A ) Tiens, entre! Je lui ai parlé de la dernière mission de C. Malthus. J’ai eu un long plan pour entreprendre la conquête du pays que Dieu m’avait promis. On peut dire qu’il avait réussi.']\n",
      "['Où en est la conquête?', 'Ils nous attendent encore quelque temps.']\n",
      "['(À B) J’ai une mission pour vous…', 'On en a assez, Nicolas!']\n",
      "['Mais s’il ne met pas son veto dans le chef spirituel qui a été désigné par Dieu, comment y a-t-il de relations?', 'Si tu ne faisais rien, au moins, la vie des hommes serait bonne et la ville serait construite selon tes propres normes.']\n",
      "['Non, je n’en ai pas beaucoup de disponibles. Mais si vous en avez un, vous avez une bonne nouvelle pour moi. (Elle sort.)', 'Quelle est la raison pour laquelle tu as interdit la Madon de ce site?']\n",
      "['C’est quoi l’agressivité?', '(Se ressaisissant) Quoi???? Ça ne se fera jamais!']\n",
      "['Ça fait longtemps, hein?', 'Si Dieu est capable de faire sortir l’eau d’une rivière sans l’embrasser, pourquoi pas lui donner aussi de l’eau?']\n",
      "['Ma mère a pensé que mes frères et moi avons dû faire un trou pour que mes petits puissent dormir. À cet endroit, il y a le puits de Jacob. Je suis tombé à genoux et j’ai embrassé Dieu, comme ça, toute la journée, juste pour me laver.', 'Les soldats se groupent autour du chef et l’aideront à prendre la ville.']\n",
      "['Sais-tu quelle a été ma prise de conscience la plus douloureuse, à moi?', 'Si tu m’as dit non, je ne peux plus m’en aller!']\n",
      "['Non. Non, c’est toi qui es sonores.', '(Riant.) Je suis dans ma voiture, juste devant ta maison!']\n",
      "['C’est de la manipulation. Je ne vois pas pourquoi tu en rajouterais…', 'Mais toi, tu n’as rien à faire pour m’aider, toi. Moi, je veux m’enlever de la présence de Dieu dans ma  jeunesse…']\n",
      "['Non.', 'C’est que B a peut-être fait une croix sur l’']\n",
      "['Quelle était ma vie, quand j’ai appris d’où venait mon sang? Je dévorais ce que je venais puiser…', 'Tu es mon ami, celui qui a créé le vide?']\n",
      "['Je suis une femme…', 'Si tu es nymphomane, pourquoi te dis-tu que c’est parce que tu t’es dopée à cette drogue que tu as finit par l’ubfermer dans le cœur?']\n",
      "['(Avec un petit sourire.) Moi, je n’ai pas arrêté de pleurer, cette petite…', 'J’ai supplié pour que tu me racontes ta dernière conquête?']\n",
      "['Tu es tellement dégoûtante?', 'C’est tout?']\n",
      "['(Se levant soudain.) Euh…', 'Eh bien, parce que je ne trouve pas le… les choses ont tellement marché!']\n",
      "['Elle a quelque chose d’animé!', '(L’interrompant.) Oui.']\n",
      "['C’est super de pouvoir avoir ce genre de miracle-là!', '(À tout le PEUPLE réuni.) Oh! Ça devrait durer plus longtemps!']\n"
     ]
    }
   ],
   "source": [
    "answered_questions = []\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model_test = GPT2LMHeadModel.from_pretrained(\"results-finetune/checkpoint-2500/\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"antoiloui/belgpt2\", model_max_length=768, pad_token='<|pad|>')\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model_test.cuda()\n",
    "\n",
    "# Generate a sample of text\n",
    "model_test.eval()\n",
    "\n",
    "for q in questions:\n",
    "    prompt = \"{}A: {}\\n B: \".format(tokenizer.bos_token, q)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model_test.generate(\n",
    "                inputs,\n",
    "                do_sample=True,   \n",
    "                top_k=50, \n",
    "                max_length=100,\n",
    "                top_p=0.95, \n",
    "                num_return_sequences=2,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decode it\n",
    "    decoded_output = []\n",
    "    for sample in output:\n",
    "        decoded_output.append(tokenizer.decode(sample, skip_special_tokens=True).split(\"B:\")[1].strip())\n",
    "    print(decoded_output)\n",
    "    \n",
    "    answered_questions.append( (q, decoded_output))\n",
    "\n",
    "del model_test\n",
    "del device\n",
    "del inputs\n",
    "torch.cuda.empty_cache()\n",
    "print(answered_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "viral-switch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Où es-tu?\\n', ['Ça fait longtemps?', 'Où suis-tu?'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answered_questions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "excessive-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/questions-answers.txt', 'w') as raw_file:\n",
    "    for (question, answers) in answered_questions:\n",
    "        seq = [ \"\\t{}\\n\".format(x) for x in answers ]\n",
    "        seq.insert(0, \"\\n{}\".format(question))\n",
    "        raw_file.writelines(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-reasoning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
